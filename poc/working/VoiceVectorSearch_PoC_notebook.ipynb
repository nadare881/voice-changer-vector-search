{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd432351",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T00:43:12.770531Z",
     "start_time": "2023-04-29T00:43:11.829889Z"
    }
   },
   "source": [
    "# 概要\n",
    "\n",
    "本notebookでは[JVS corpus](https://sites.google.com/site/shinnosuketakamichi/research-topics/jvs_corpus)のデータセットに対し、[pyannote](https://github.com/pyannote/pyannote-audio)を用いて話者情報のembeddingを取得し、類似した話者のデータセットを検索できるかを検証する。\n",
    "\n",
    "JVS corpusの中には複数のデータセットがある。以下のデータセットそれぞれに対してembeddingを取得する。\n",
    "\n",
    "- 話者間で共通する読み上げ音声100 発話を集めたparallel100\n",
    "- 話者間で全く異なる読み上げ音声 30 発話を集めたnonpara30\n",
    "\n",
    "話者ごとにembeddingの平均をとり、l2正規化したものを話者embeddingとする。\n",
    "\n",
    "これに対し、[embedding projector](https://projector.tensorflow.org/)によるプロットと、[Word Tour](https://arxiv.org/abs/2205.01954)による一次元上での並び替えを行い、類似した音声を取得できるか確認する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b07991",
   "metadata": {},
   "source": [
    "# 設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853dfd5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T05:27:20.784903Z",
     "start_time": "2023-04-30T05:27:20.769905Z"
    }
   },
   "source": [
    "## ライブラリのimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6438f3cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T06:19:36.671369Z",
     "start_time": "2023-04-30T06:19:32.852782Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ortools.constraint_solver import pywrapcp\n",
    "from ortools.constraint_solver import routing_enums_pb2\n",
    "from pyannote.audio import Model, Inference\n",
    "from IPython.display import Audio\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d21dbb9",
   "metadata": {},
   "source": [
    "## ディレクトリの配置\n",
    "\n",
    "poc\n",
    "\n",
    " |--dataset/jvs/jvs_ver1\n",
    "\n",
    " |--working/VoiceVectorSearch_PoC_notebook.ipynb\n",
    "\n",
    " |--output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1ea2f70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T06:19:36.687474Z",
     "start_time": "2023-04-30T06:19:36.672372Z"
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"../output/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1575c7",
   "metadata": {},
   "source": [
    "## HuggingFaceよりembeddingモデルのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fadfe90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T06:19:37.494854Z",
     "start_time": "2023-04-30T06:19:36.688479Z"
    }
   },
   "outputs": [],
   "source": [
    "token = \"your-huggingface-token\"\n",
    "model = Model.from_pretrained(\"pyannote/embedding\", \n",
    "                              use_auth_token=token)\n",
    "inference = Inference(model, window=\"whole\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b9cc99",
   "metadata": {},
   "source": [
    "# parrallel100での検証"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f7d75a",
   "metadata": {},
   "source": [
    "共通する読み上げ音声から得られたembeddingの比較により、類似する音声を取得できるかを調べる。\n",
    "\n",
    "これによりRVC等のモデルに対し共通のコーパスで変換した音声を用いてモデルを検索できるか確かめる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6850d8",
   "metadata": {},
   "source": [
    "## embeddingの取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f89eac8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T06:22:27.626205Z",
     "start_time": "2023-04-30T06:19:37.495855Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec8d86d6892040bf948e63c328a9f547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../dataset/jvs/jvs_ver1/jvs030/parallel100/wav24kHz16bit/VOICEACTRESS100_045.wav is not exist\n",
      "../dataset/jvs/jvs_ver1/jvs074/parallel100/wav24kHz16bit/VOICEACTRESS100_094.wav is not exist\n",
      "../dataset/jvs/jvs_ver1/jvs089/parallel100/wav24kHz16bit/VOICEACTRESS100_019.wav is not exist\n"
     ]
    }
   ],
   "source": [
    "# embedding の抽出\n",
    "parallel_embs = []\n",
    "for person in tqdm(range(1, 101)):\n",
    "    emb = []\n",
    "    dir_ = f\"../dataset/jvs/jvs_ver1/jvs{person:03}/parallel100/wav24kHz16bit/\"\n",
    "    for i in range(1, 101):\n",
    "        path = dir_ + f'VOICEACTRESS100_{i:03}.wav'\n",
    "        if os.path.exists(path):\n",
    "            emb.append(inference(path))\n",
    "        else:\n",
    "            print(path, \"is not exist\")\n",
    "            emb.append(np.zeros(512))\n",
    "    parallel_embs.append(np.stack(emb, axis=0))\n",
    "parallel_embs = np.stack(parallel_embs, axis=0)\n",
    "\n",
    "# 話者ごとにembeddingの和をとり、l2正規化を行う\n",
    "parallel_embs_normed = parallel_embs.sum(axis=1)\n",
    "parallel_embs_normed = parallel_embs_normed / np.linalg.norm(parallel_embs_normed, ord=2, axis=1, keepdims=True)\n",
    "\n",
    "# データの書き込み\n",
    "pd.read_csv(\"../dataset/jvs/jvs_ver1/gender_f0range.txt\", delimiter=\" \").to_csv(\"../output/jvs_speaker_meta.tsv\", sep=\"\\t\", index=None, encoding=\"utf-8\")\n",
    "pd.DataFrame(parallel_embs_normed).astype(np.float16).to_csv(\"../output/jvs_parallel100_emb.tsv\", sep=\"\\t\", index=None, header=None, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cce1456",
   "metadata": {},
   "source": [
    "## WordTour による一次元化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de1537f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T06:22:27.769717Z",
     "start_time": "2023-04-30T06:22:27.629206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jvs001', 'jvs070', 'jvs013', 'jvs005', 'jvs012', 'jvs022', 'jvs071', 'jvs028', 'jvs037', 'jvs086', 'jvs068', 'jvs078', 'jvs081', 'jvs074', 'jvs054', 'jvs098', 'jvs097', 'jvs023', 'jvs100', 'jvs020', 'jvs033', 'jvs079', 'jvs002', 'jvs091', 'jvs059', 'jvs056', 'jvs096', 'jvs072', 'jvs007', 'jvs092', 'jvs014', 'jvs017', 'jvs055', 'jvs067', 'jvs066', 'jvs082', 'jvs065', 'jvs084', 'jvs064', 'jvs095', 'jvs030', 'jvs090', 'jvs015', 'jvs010', 'jvs018', 'jvs058', 'jvs069', 'jvs019', 'jvs024', 'jvs025', 'jvs060', 'jvs094', 'jvs085', 'jvs004', 'jvs093', 'jvs026', 'jvs063', 'jvs062', 'jvs061', 'jvs029', 'jvs036', 'jvs083', 'jvs043', 'jvs038', 'jvs053', 'jvs039', 'jvs051', 'jvs057', 'jvs027', 'jvs016', 'jvs008', 'jvs040', 'jvs035', 'jvs032', 'jvs041', 'jvs089', 'jvs042', 'jvs076', 'jvs031', 'jvs050', 'jvs003', 'jvs073', 'jvs046', 'jvs047', 'jvs021', 'jvs009', 'jvs088', 'jvs034', 'jvs006', 'jvs048', 'jvs045', 'jvs080', 'jvs049', 'jvs044', 'jvs075', 'jvs011', 'jvs077', 'jvs087', 'jvs099', 'jvs052']\n"
     ]
    }
   ],
   "source": [
    "from ortools.constraint_solver import pywrapcp\n",
    "from ortools.constraint_solver import routing_enums_pb2\n",
    "\n",
    "# コサイン距離((1 - コサイン類似度)/2)を用いて巡回セールスマン問題を解き、一次元上にembeddingを並べる\n",
    "\n",
    "data = {}\n",
    "data[\"distance_matrix\"] = ((1 - np.einsum(\"nd,md->nm\", parallel_embs_normed, parallel_embs_normed)) / 2. * (1. - np.eye(100)) * 1e9).astype(np.int64).tolist()\n",
    "data[\"num_vehicles\"] = 1\n",
    "data[\"depot\"] = 0\n",
    "\n",
    "\n",
    "def distance_callback(from_index, to_index):\n",
    "    \"\"\"Returns the distance between the two nodes.\"\"\"\n",
    "    # Convert from routing variable Index to distance matrix NodeIndex.\n",
    "    from_node = manager.IndexToNode(from_index)\n",
    "    to_node = manager.IndexToNode(to_index)\n",
    "    return data['distance_matrix'][from_node][to_node]\n",
    "\n",
    "def get_routes(solution, routing, manager):\n",
    "    \"\"\"Get vehicle routes from a solution and store them in an array.\"\"\"\n",
    "    # Get vehicle routes and store them in a two dimensional array whose\n",
    "    # i,j entry is the jth location visited by vehicle i along its route.\n",
    "    routes = []\n",
    "    for route_nbr in range(routing.vehicles()):\n",
    "        index = routing.Start(route_nbr)\n",
    "        route = [manager.IndexToNode(index)]\n",
    "        while not routing.IsEnd(index):\n",
    "            index = solution.Value(routing.NextVar(index))\n",
    "            route.append(manager.IndexToNode(index))\n",
    "        routes.append(route)\n",
    "    return routes[0]\n",
    "\n",
    "manager = pywrapcp.RoutingIndexManager(len(data['distance_matrix']),\n",
    "                                       data['num_vehicles'], data['depot'])\n",
    "routing = pywrapcp.RoutingModel(manager)\n",
    "transit_callback_index = routing.RegisterTransitCallback(distance_callback)\n",
    "routing.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)\n",
    "search_parameters = pywrapcp.DefaultRoutingSearchParameters()\n",
    "search_parameters.first_solution_strategy = (\n",
    "    routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC)\n",
    "\n",
    "solution = routing.SolveWithParameters(search_parameters)\n",
    "routes = [f\"jvs{x+1:03}\" for x in get_routes(solution, routing, manager)[:-1]]\n",
    "print(routes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7475e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T05:58:16.575292Z",
     "start_time": "2023-04-30T05:58:16.557169Z"
    }
   },
   "source": [
    "## 音声の聞き比べ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edd81c2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T06:26:05.735927Z",
     "start_time": "2023-04-30T06:26:05.728926Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "# iで話者、xでスクリプトを選択する\n",
    "i, x = 0, 1\n",
    "# Audio(\"../dataset/jvs/jvs_ver1/{}/parallel100/wav24kHz16bit/VOICEACTRESS100_{:03}.wav\".format(routes[i], x), autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118178b8",
   "metadata": {},
   "source": [
    "# nonpara30での検証"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df261c5",
   "metadata": {},
   "source": [
    "話者間で異なる読み上げ音声から得られたembeddingの比較により、類似する音声を取得できるかを調べる。\n",
    "\n",
    "これにより任意の音声から類似した音声のデータセットを検索できるか確かめる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa54265",
   "metadata": {},
   "source": [
    "## embeddingの取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6362bcf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T06:23:31.751994Z",
     "start_time": "2023-04-30T06:22:27.884742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb0d157debbf4002b684b2a344b7ef07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# embedding の抽出\n",
    "nonpara_embs = []\n",
    "for person in tqdm(range(1, 101)):\n",
    "    emb = []\n",
    "    dir_ = f\"../dataset/jvs/jvs_ver1/jvs{person:03}/nonpara30/wav24kHz16bit/\"\n",
    "    for file in os.listdir(dir_):\n",
    "        if not file.endswith(\".wav\"):\n",
    "            continue\n",
    "        path = dir_ + file\n",
    "        emb.append(inference(path))\n",
    "    nonpara_embs.append(np.stack(emb, axis=0))\n",
    "nonpara_embs = np.stack(nonpara_embs, axis=0)\n",
    "\n",
    "# 話者ごとにembeddingの和をとり、l2正規化を行う\n",
    "nonpara_embs_normed = nonpara_embs.sum(axis=1)\n",
    "nonpara_embs_normed = nonpara_embs_normed / np.linalg.norm(nonpara_embs_normed, ord=2, axis=1, keepdims=True)\n",
    "\n",
    "# データの書き込み\n",
    "pd.DataFrame(nonpara_embs_normed).astype(np.float16).to_csv(\"../output/jvs_nonpara30_emb.tsv\", sep=\"\\t\", index=None, header=None, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39de351",
   "metadata": {},
   "source": [
    "## Word Tourによる一次元化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "203a6a3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T06:23:31.878119Z",
     "start_time": "2023-04-30T06:23:31.756996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jvs001', 'jvs052', 'jvs099', 'jvs089', 'jvs041', 'jvs054', 'jvs074', 'jvs081', 'jvs076', 'jvs097', 'jvs023', 'jvs031', 'jvs028', 'jvs071', 'jvs022', 'jvs042', 'jvs027', 'jvs057', 'jvs051', 'jvs094', 'jvs004', 'jvs085', 'jvs093', 'jvs082', 'jvs039', 'jvs019', 'jvs024', 'jvs025', 'jvs072', 'jvs092', 'jvs007', 'jvs017', 'jvs055', 'jvs066', 'jvs014', 'jvs067', 'jvs053', 'jvs065', 'jvs038', 'jvs060', 'jvs061', 'jvs029', 'jvs096', 'jvs056', 'jvs059', 'jvs091', 'jvs002', 'jvs043', 'jvs083', 'jvs036', 'jvs084', 'jvs026', 'jvs069', 'jvs062', 'jvs063', 'jvs064', 'jvs095', 'jvs030', 'jvs090', 'jvs015', 'jvs010', 'jvs058', 'jvs018', 'jvs016', 'jvs008', 'jvs040', 'jvs035', 'jvs087', 'jvs077', 'jvs011', 'jvs032', 'jvs005', 'jvs013', 'jvs070', 'jvs009', 'jvs088', 'jvs073', 'jvs078', 'jvs046', 'jvs047', 'jvs003', 'jvs050', 'jvs048', 'jvs034', 'jvs006', 'jvs021', 'jvs098', 'jvs068', 'jvs075', 'jvs044', 'jvs049', 'jvs079', 'jvs086', 'jvs037', 'jvs045', 'jvs080', 'jvs033', 'jvs020', 'jvs100', 'jvs012']\n"
     ]
    }
   ],
   "source": [
    "from ortools.constraint_solver import pywrapcp\n",
    "from ortools.constraint_solver import routing_enums_pb2\n",
    "\n",
    "# コサイン距離((1 - コサイン類似度)/2)を用いて巡回セールスマン問題を解き、一次元上にembeddingを並べる\n",
    "\n",
    "data = {}\n",
    "data[\"distance_matrix\"] = ((1 - np.einsum(\"nd,md->nm\", nonpara_embs_normed, nonpara_embs_normed)) / 2. * (1. - np.eye(100)) * 1e9).astype(np.int64).tolist()\n",
    "data[\"num_vehicles\"] = 1\n",
    "data[\"depot\"] = 0\n",
    "\n",
    "\n",
    "def distance_callback(from_index, to_index):\n",
    "    \"\"\"Returns the distance between the two nodes.\"\"\"\n",
    "    # Convert from routing variable Index to distance matrix NodeIndex.\n",
    "    from_node = manager.IndexToNode(from_index)\n",
    "    to_node = manager.IndexToNode(to_index)\n",
    "    return data['distance_matrix'][from_node][to_node]\n",
    "\n",
    "def get_routes(solution, routing, manager):\n",
    "    \"\"\"Get vehicle routes from a solution and store them in an array.\"\"\"\n",
    "    # Get vehicle routes and store them in a two dimensional array whose\n",
    "    # i,j entry is the jth location visited by vehicle i along its route.\n",
    "    routes = []\n",
    "    for route_nbr in range(routing.vehicles()):\n",
    "        index = routing.Start(route_nbr)\n",
    "        route = [manager.IndexToNode(index)]\n",
    "        while not routing.IsEnd(index):\n",
    "            index = solution.Value(routing.NextVar(index))\n",
    "            route.append(manager.IndexToNode(index))\n",
    "        routes.append(route)\n",
    "    return routes[0]\n",
    "\n",
    "manager = pywrapcp.RoutingIndexManager(len(data['distance_matrix']),\n",
    "                                       data['num_vehicles'], data['depot'])\n",
    "routing = pywrapcp.RoutingModel(manager)\n",
    "transit_callback_index = routing.RegisterTransitCallback(distance_callback)\n",
    "routing.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)\n",
    "search_parameters = pywrapcp.DefaultRoutingSearchParameters()\n",
    "search_parameters.first_solution_strategy = (\n",
    "    routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC)\n",
    "\n",
    "solution = routing.SolveWithParameters(search_parameters)\n",
    "routes = [f\"jvs{x+1:03}\" for x in get_routes(solution, routing, manager)[:-1]]\n",
    "print(routes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be31303e",
   "metadata": {},
   "source": [
    "## 音声の聞き比べ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d0a002a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T06:26:12.559740Z",
     "start_time": "2023-04-30T06:26:12.539976Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "# iで話者、xでスクリプトを選択する\n",
    "i, x = 0, 1\n",
    "# Audio(\"../dataset/jvs/jvs_ver1/{}/parallel100/wav24kHz16bit/VOICEACTRESS100_{:03}.wav\".format(routes[i], x), autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dafa80",
   "metadata": {},
   "source": [
    "# embedding projectorへの反映"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fd88ebd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T06:23:31.926183Z",
     "start_time": "2023-04-30T06:23:31.917200Z"
    }
   },
   "outputs": [],
   "source": [
    "config_json = {\"embeddings\": [{\"tensorName\": \"Parallel100\",\n",
    "                               \"tensorShape\": [100, 512],\n",
    "                               \"tensorPath\": \"https://raw.githubusercontent.com/nadare881/voice-changer-vector-search/main/poc/output/jvs_parallel100_emb.tsv\",\n",
    "                               \"metadataPath\": \"https://raw.githubusercontent.com/nadare881/voice-changer-vector-search/main/poc/output/jvs_speaker_meta.tsv\"},\n",
    "                              {\"tensorName\": \"Nonpara30\",\n",
    "                               \"tensorShape\": [100, 512],\n",
    "                               \"tensorPath\": \"https://raw.githubusercontent.com/nadare881/voice-changer-vector-search/main/poc/output/jvs_nonpara30_emb.tsv\",\n",
    "                               \"metadataPath\": \"https://raw.githubusercontent.com/nadare881/voice-changer-vector-search/main/poc/output/jvs_speaker_meta.tsv\"}]}\n",
    "with open(\"../output/embeddding_projector_config.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(config_json, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10675d0",
   "metadata": {},
   "source": [
    "結果は[こちら(embedding projector)](https://projector.tensorflow.org/?config=https://raw.githubusercontent.com/nadare881/voice-changer-vector-search/main/poc/output/embeddding_projector_config.json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
